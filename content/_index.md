---
title: "Alan Jeffares"
description: "Alan Jeffares"
---

# Alan Jeffares

  <div class="profile-pic">
    <img src="/images/profile_pic.jpeg" alt="profile" />
  </div>

I'm a 3rd year Machine Learning PhD student at the¬†University of Cambridge¬†in the [Department of Applied Mathematics](http://www.damtp.cam.ac.uk/). I am interested in building a better understanding of empirical phenomena in deep learning (e.g. double descent, optimization heuristics) and developing methodological advances from these insights (e.g. deep ensembles, mixture-of-experts). I hold an MSc in Machine Learning from [University College London](https://www.ucl.ac.uk/) and a BSc in Statistics from [University College Dublin](https://www.ucd.ie/). I have previously worked as a Data Scientist at [Accenture's global center for R&D innovation](https://www.accenture.com/il-en/services/about/innovation-hub-the-dock) and in the [Insight Research Center for Data Analytics](https://www.insight-centre.org/). Email at: *aj659 [at] cam [dot] ac [dot] uk*.


{{% center %}}
[[twitter](https://twitter.com/Jeffaresalan)] [[scholar](https://scholar.google.com/citations?user=e65kJ08AAAAJ&hl=en)] [[github](https://github.com/alanjeffares)] [[linkedin](https://linkedin.com/in/alanjeffares)]
{{% /center %}}

---

## üóûÔ∏è News üóûÔ∏è

* <span class="date">September 2024</span> &#8594; New paper accepted at [**NeurIPS2024**](https://nips.cc/)! This [paper](https://openreview.net/forum?id=NhucGZtikE) develops a simplified model of a neural network to uncover insights into double descent, grokking, gradient boosting, and linear mode connectivity. Check out our Twitter threads for a bite-sized summary -- [part I](https://x.com/AliciaCurth/status/1858497131796967723), [part II](https://x.com/AliciaCurth/status/1859269161911713850), & part III (coming soon). 
  
* <span class="date">June 2024</span> &#8594; Excited to have begun my internship at Microsoft Research Redmond for the summer where I'll be working on discrete optimization and mixture-of-expert models under the brilliant [Lucas Liu](https://liyuanlucasliu.github.io/) and the deep learning team.

* <span class="date">May 2024</span> &#8594; _New paper accepted at_ [**ICML2024**](https://icml.cc/)_!_ This [paper](https://arxiv.org/abs/2406.03258) deals with the task of estimating well-calibrated prediction intervals and proposes a simple alternative to quantile regression that relaxes the implicit assumption of a symmetric noise distribution. I will also present "Looking at Deep Learning Phenomena Through a Telescoping Lens" at the [HiLD workshop](https://sites.google.com/view/hidimlearning/home). 

* <span class="date">Sep 2023</span> &#8594; _Two papers accepted for_ [**NeurIPS2023**](https://nips.cc/)_!_ One **oral** (top 0.5% of submissions) that provides an alternative take on double descent suggesting that it may not be so contradictory from classic statistical notions of model complexity. Then, a **poster** that investigates if deep ensembles can be trained jointly rather than independently.

* <span class="date">Jan 2023</span> &#8594; _Two papers accepted!_ ü•≥ One at [**AISTATS23**](https://virtual.aistats.org/Conferences/2023) ([[paper](https://proceedings.mlr.press/v206/seedat23a.html)]) and one at [**ICLR23**](https://iclr.cc/) ([[paper](https://openreview.net/forum?id=n6H86gW8u0d)]). These papers explore self-supervised learning for conformal prediction and a new regularizer for neural networks, respectively. I look forward to presenting these with my co-authors!

* <span class="date">April 2022</span> &#8594; I have officially started a PhD in Machine Learning in the University of Cambridge under the supervision of Mihaela van der Schaar! 

* <span class="date">Jan 2022</span> &#8594; _First paper accepted!_ üéâ Work done during my masters thesis under the supervision of Timos Moraitis and Pontus Stenetorp has been accepted as a **spotlight** (top 5% of submissions) at **ICLR22**. This [paper](https://openreview.net/pdf?id=iMH1e5k7n3L) took a neuroscience-inspired approach to improve the accuracy-efficiency trade-off in RNNs. 

* <span class="date">Dec 2021</span> &#8594; _Graduated_ üéì I have officially graduated with an MSc in Machine Learning from UCL. I was also a recipient of a Dean's list award for "outstanding academic performance". 


---

## üìö Research üìö

Please find some of my publications below (a more up-to-date list can be found on¬†[google scholar](https://scholar.google.com/citations?user=e65kJ08AAAAJ&hl=en)).

"\*" denotes equal contribution.

### Preprints

- A. Curth, <mark>A. Jeffares</mark>, M. van der Schaar. *Why do Random Forests Work? Understanding Tree Ensembles as Self-Regularizing Adaptive Smoothers*. [[preprint]](https://arxiv.org/abs/2402.01502)

### Conferences

- <mark>A. Jeffares*</mark>, A. Curth, M. van der Schaar. *Deep Learning Through A Telescoping Lens: A Simple Model Provides Empirical Insights On Grokking, Gradient Boosting & Beyond*. **NeurIPS, 2024**. [[paper]](https://openreview.net/forum?id=NhucGZtikE)
- T. Pouplin*, <mark>A. Jeffares*</mark>, N. Seedat, M. van der Schaar. *Relaxed Quantile Regression: Prediction Intervals for Asymmetric Noise*. **ICML 2024** [[paper]](https://arxiv.org/abs/2406.03258) [[code]](https://github.com/TPouplin/RQR)
- <mark>A. Jeffares*</mark>, A. Curth, M. van der Schaar. *Looking at Deep Learning Phenomena Through a Telescoping Lens*. **HiLD workshop @ ICML**. [[paper]](https://openreview.net/forum?id=hJ7hfAzsuT)
- A. Curth*, <mark>A. Jeffares*</mark>, M. van der Schaar. *A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning*. **NeurIPS, 2023 - Oral (top 0.5%)**. [[paper]](https://openreview.net/forum?id=O0Lz8XZT2b) [[code]](https://github.com/alanjeffares/not-double-descent)
- <mark>A. Jeffares</mark>, T. Liu, J .Crabb√©, M. van der Schaar. *Joint Training of Deep Ensembles Fails Due to Learner Collusion*. **NeurIPS, 2023** [[paper]](https://openreview.net/forum?id=WpGLxnOWhn) [[code]](https://github.com/alanjeffares/joint-ensembles)
- N. Seedat*, <mark>A. Jeffares*</mark>, F. Imrie, M. van der Schaar. *Improving Adaptive Conformal Prediction Using Self-Supervised Learning*. **AISTATS, 2023** [[paper]](https://proceedings.mlr.press/v206/seedat23a.html)¬†[[code]](https://github.com/seedatnabeel/SSCP)
- <mark>A. Jeffares*</mark>, T. Liu*, J. Crabb√©, F. Imrie, M. van der Schaar. *TANGOS: Regularizing Tabular Neural Networks through Gradient Orthogonalization and Specialization*. **ICLR, 2023** [[paper]](https://openreview.net/forum?id=n6H86gW8u0d)¬†[[code]](https://github.com/alanjeffares/TANGOS)
- <mark>A. Jeffares</mark>, Q. Guo, P. Stenetorp, T. Moraitis. *Spike-inspired rank coding for fast and accurate recurrent neural networks*. **ICLR, 2022 - Spotlight (top 5%)**. [[paper]](https://openreview.net/pdf?id=iMH1e5k7n3L)¬†[[code]](https://github.com/NeuromorphicComputing/RankCoding)


